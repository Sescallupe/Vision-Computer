{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e123d7-98ec-4b96-8338-8b2857895bc5",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Mosaico de Fotos\n",
    "\n",
    " En este notebook, aprenderemos cómo unir dos imágenes para crear un mosaico o panorama. Utilizaremos técnicas de detección de características, coincidencia de puntos clave y homografía para alinear y transformar las imágenes, creando una imagen panorámica final.\n",
    "\n",
    " El flujo del proceso incluye:\n",
    " 1. Leer y preparar las imágenes.\n",
    " 2. Detectar y describir los puntos clave en ambas imágenes.\n",
    " 3. Coincidir los puntos clave entre las dos imágenes.\n",
    " 4. Calcular la homografía para alinear las imágenes.\n",
    " 5. Transformar y unir las imágenes.\n",
    " 6. Recortar la imagen resultante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a71e2bf-4cc8-40d8-939a-f66c5f1d342e",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# %%\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcv2\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import imageio\n",
    "import imutils\n",
    "cv2.ocl.setUseOpenCL(False)  # Desactiva el uso de OpenCL para evitar problemas de compatibilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf933978-bf88-4925-bb12-61551ae4ef6f",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Selección de método de detección y coincidencia de características\n",
    "\n",
    " Elegimos un extractor de características (por ejemplo, 'orb') y un método de coincidencia de características (por ejemplo, 'bf' para Brute-Force)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e5196a-7696-48f4-8c03-d61ebe705a87",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Selección del método de extracción de características\n",
    "feature_extractor = 'orb'  # Opciones: 'sift', 'surf', 'brisk', 'orb'\n",
    "feature_matching = 'bf'    # Opciones de coincidencia: 'bf' para Brute-Force o 'knn' para KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a71f4de-b9a0-40b6-8749-0238cb07f344",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Leer y convertir las imágenes a escala de grises\n",
    "\n",
    " Leemos las imágenes (la imagen de consulta y la imagen de entrenamiento) y las convertimos a escala de grises para simplificar el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a634ec9-1aa8-4708-9e87-2e2b2085572e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Cargar y convertir las imágenes a escala de grises\n",
    "trainImg = imageio.imread('/content/drive/MyDrive/panorama(train).jpg')\n",
    "trainImg_gray = cv2.cvtColor(trainImg, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "queryImg = imageio.imread('/content/drive/MyDrive/panorama(query).jpg')\n",
    "queryImg_gray = cv2.cvtColor(queryImg, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "# Visualización de las imágenes cargadas\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, constrained_layout=False, figsize=(10,9))\n",
    "ax1.imshow(queryImg, cmap=\"gray\")\n",
    "ax1.set_xlabel(\"Imagen de consulta (Query)\", fontsize=14)\n",
    "ax2.imshow(trainImg, cmap=\"gray\")\n",
    "ax2.set_xlabel(\"Imagen de entrenamiento (Train)\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c80e7b-a1bf-4c60-9ada-a4ee44e89ce4",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Detección y descripción de características\n",
    "\n",
    " La función `detectAndDescribe` detecta puntos clave y calcula descriptores en una imagen usando el método especificado. Dependiendo de la selección (`sift`, `surf`, `brisk`, `orb`), se usa un detector/descripción diferente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a55a820-cac9-475b-aee0-cd6624363ec3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def detectAndDescribe(image, method=None):\n",
    "    \"\"\"\n",
    "    Detectar y describir puntos clave y descriptores de una imagen usando el método especificado\n",
    "    \"\"\"\n",
    "    assert method is not None, \"Debes definir un método de detección de características: 'sift', 'surf', 'brisk', 'orb'\"\n",
    "    \n",
    "    # Selección del método de detección\n",
    "    if method == 'sift':\n",
    "        descriptor = cv2.xfeatures2d.SIFT_create()\n",
    "    elif method == 'surf':\n",
    "        descriptor = cv2.xfeatures2d.SURF_create()\n",
    "    elif method == 'brisk':\n",
    "        descriptor = cv2.BRISK_create()\n",
    "    elif method == 'orb':\n",
    "        descriptor = cv2.ORB_create()\n",
    "        \n",
    "    # Detección de puntos clave y cálculo de descriptores\n",
    "    (kps, features) = descriptor.detectAndCompute(image, None)\n",
    "    \n",
    "    return (kps, features)\n",
    "\n",
    "# Detección de puntos clave y descriptores en ambas imágenes\n",
    "kpsA, featuresA = detectAndDescribe(trainImg_gray, method=feature_extractor)\n",
    "kpsB, featuresB = detectAndDescribe(queryImg_gray, method=feature_extractor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8caa4-ad99-4545-869b-f188d1432a37",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Visualización de los puntos clave detectados\n",
    "\n",
    " Mostramos los puntos clave detectados en ambas imágenes para entender la distribución de características detectadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed9bbd1-607c-4d68-920b-527bfa605a70",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Visualizar puntos clave en ambas imágenes\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(20,8), constrained_layout=False)\n",
    "ax1.imshow(cv2.drawKeypoints(trainImg_gray, kpsA, None, color=(0, 255, 0)))\n",
    "ax1.set_xlabel(\"(a) Imagen de entrenamiento\", fontsize=14)\n",
    "ax2.imshow(cv2.drawKeypoints(queryImg_gray, kpsB, None, color=(0, 255, 0)))\n",
    "ax2.set_xlabel(\"(b) Imagen de consulta\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea19417-d219-4963-9f05-7fb06b2d1ebc",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Creación del Emparejador de Características (Matcher)\n",
    "\n",
    " La función `createMatcher` crea y devuelve un objeto de emparejamiento de características en función del método de detección elegido."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fa5cb-596e-4a6b-80c5-d889e386c21b",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def createMatcher(method, crossCheck):\n",
    "    \"\"\"\n",
    "    Crear y devolver un objeto Matcher\n",
    "    \"\"\"\n",
    "    if method in ['sift', 'surf']:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck=crossCheck)\n",
    "    elif method in ['orb', 'brisk']:\n",
    "        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=crossCheck)\n",
    "    return bf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89df677b-5cf6-40dd-b687-269417a1b15c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Coincidencia de Puntos Clave con Brute-Force\n",
    "\n",
    " Usamos el emparejador Brute-Force para encontrar los puntos clave más cercanos entre las dos imágenes. La coincidencia por Brute-Force compara cada descriptor en la primera imagen con todos los descriptores en la segunda, seleccionando la coincidencia más cercana."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a51318f-f524-4ea6-a46b-186dbeb27bd1",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def matchKeyPointsBF(featuresA, featuresB, method):\n",
    "    bf = createMatcher(method, crossCheck=True)\n",
    "        \n",
    "    # Coincidir descriptores\n",
    "    best_matches = bf.match(featuresA, featuresB)\n",
    "    \n",
    "    # Ordenar las coincidencias en orden de menor a mayor distancia\n",
    "    rawMatches = sorted(best_matches, key=lambda x: x.distance)\n",
    "    print(\"Número de coincidencias (Brute Force):\", len(rawMatches))\n",
    "    return rawMatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2edeaccd-1c2f-4a4e-8a98-137289ff5ab1",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Coincidencia de Puntos Clave con KNN y Prueba de Razón\n",
    "\n",
    " La función `matchKeyPointsKNN` utiliza el emparejador K-Nearest Neighbor (KNN) con la prueba de razón de Lowe para mejorar la precisión y reducir coincidencias falsas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae20fd-575b-4561-8bdb-49f41900b678",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def matchKeyPointsKNN(featuresA, featuresB, ratio, method):\n",
    "    bf = createMatcher(method, crossCheck=False)\n",
    "    rawMatches = bf.knnMatch(featuresA, featuresB, 2)\n",
    "    print(\"Número de coincidencias (KNN):\", len(rawMatches))\n",
    "    matches = []\n",
    "\n",
    "    # Aplicar la prueba de razón\n",
    "    for m, n in rawMatches:\n",
    "        if m.distance < n.distance * ratio:\n",
    "            matches.append(m)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd0ab3-e20d-4478-9ab9-04fc37609ee8",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Selección y Visualización de Coincidencias\n",
    "\n",
    " Elegimos el tipo de coincidencia (Brute-Force o KNN) y mostramos las primeras coincidencias entre las dos imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3954e3-4a7e-4ee0-b116-c5e4696114e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "print(f\"Usando: {feature_matching} para el emparejamiento de características\")\n",
    "\n",
    "fig = plt.figure(figsize=(20,8))\n",
    "if feature_matching == 'bf':\n",
    "    matches = matchKeyPointsBF(featuresA, featuresB, method=feature_extractor)\n",
    "    img3 = cv2.drawMatches(trainImg, kpsA, queryImg, kpsB, matches[:100],\n",
    "                           None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "elif feature_matching == 'knn':\n",
    "    matches = matchKeyPointsKNN(featuresA, featuresB, ratio=0.75, method=feature_extractor)\n",
    "    img3 = cv2.drawMatches(trainImg, kpsA, queryImg, kpsB, np.random.choice(matches, 100),\n",
    "                           None, flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n",
    "\n",
    "plt.imshow(img3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5b012d-3a8c-4c9e-8dc3-c9c0e4e24bed",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Cálculo de la Homografía\n",
    "\n",
    " Usamos la función `getHomography` para calcular la matriz de homografía que transforma los puntos de la imagen de entrenamiento para alinearse con la imagen de consulta. Esta matriz es crucial para la transformación y unión de las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1f2aba-bc5d-4479-9c76-6e32fbd05b57",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh):\n",
    "    kpsA = np.float32([kp.pt for kp in kpsA])\n",
    "    kpsB = np.float32([kp.pt for kp in kpsB])\n",
    "    \n",
    "    if len(matches) > 4:\n",
    "        # Crear conjuntos de puntos\n",
    "        ptsA = np.float32([kpsA[m.queryIdx] for m in matches])\n",
    "        ptsB = np.float32([kpsB[m.trainIdx] for m in matches])\n",
    "        \n",
    "        # Calcular la homografía usando RANSAC\n",
    "        (H, status) = cv2.findHomography(ptsA, ptsB, cv2.RANSAC, reprojThresh)\n",
    "        return (matches, H, status)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Cálculo de la homografía\n",
    "M = getHomography(kpsA, kpsB, featuresA, featuresB, matches, reprojThresh=4)\n",
    "if M is None:\n",
    "    print(\"Error en la homografía\")\n",
    "(matches, H, status) = M\n",
    "print(\"Matriz de Homografía:\\n\", H)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60d06d7-32dc-4ea1-8b76-6907d71b9ad5",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Aplicación de la Transformación y Unión de Imágenes\n",
    "\n",
    " Usamos la matriz de homografía para transformar la imagen de entrenamiento y alinearla con la imagen de consulta, creando el panorama."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f38802ae-b250-4d25-a6a2-c7b446fda9ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Aplicar la transformación perspectiva\n",
    "width = trainImg.shape[1] + queryImg.shape[1]\n",
    "height = trainImg.shape[0] + queryImg.shape[0]\n",
    "\n",
    "result = cv2.warpPerspective(trainImg, H, (width, height))\n",
    "result[0:queryImg.shape[0], 0:queryImg.shape[1]] = queryImg\n",
    "\n",
    "# Mostrar el mosaico resultante\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffb257c-1f5b-490a-b2f4-10ccf9cfe52a",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Recorte de la Imagen Final\n",
    "\n",
    " Convertimos la imagen resultante a escala de grises, encontramos los contornos y recortamos la imagen para eliminar áreas en blanco."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d30c9-ebf7-4efe-823c-26b0158bfe41",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Transformar el mosaico a escala de grises y aplicar umbral\n",
    "gray = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "# Encontrar contornos\n",
    "cnts = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "cnts = imutils.grab_contours(cnts)\n",
    "\n",
    "# Obtener el contorno máximo\n",
    "c = max(cnts, key=cv2.contourArea)\n",
    "\n",
    "# Crear un rectángulo delimitador y recortar la imagen\n",
    "(x, y, w, h) = cv2.boundingRect(c)\n",
    "result = result[y:y + h, x:x + w]\n",
    "\n",
    "# Mostrar la imagen final recortada\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945af622-ed9f-4318-9b38-85c4fc6cc994",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " # Ejemplo de Mosaico de Imágenes\n",
    "\n",
    " Este código utiliza métodos de detección de características, coincidencia de puntos clave, homografía y transformación de perspectiva para unir varias imágenes en un mosaico.\n",
    "\n",
    " El flujo principal del proceso es el siguiente:\n",
    " 1. **Detección de puntos clave y coincidencia**: Usamos SIFT para detectar y coincidir puntos clave entre las imágenes.\n",
    " 2. **Cálculo de la homografía**: A partir de los puntos coincidentes, calculamos la matriz de homografía para alinear las imágenes.\n",
    " 3. **Transformación y combinación de imágenes**: Transformamos una imagen para alinearla con otra y luego las unimos en un mosaico.\n",
    " 4. **Recorte de la imagen resultante**: Eliminamos los bordes negros para obtener un mosaico limpio.\n",
    "\n",
    " Empezamos cargando las librerías necesarias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03a6844-e6f1-431d-814f-acc6d49c1791",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac33176-4078-4a4d-8f3e-299ae0c2dfcc",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Configuración de la visualización en Jupyter\n",
    "\n",
    " Configuramos `matplotlib` para mostrar gráficos en línea en el notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7819aeaf-cc9a-4033-b959-17feee3d4e9e",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3911cfa-9ee7-496e-a37d-e4f4860a3d93",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Función `getHomo`: Cálculo de Homografía\n",
    "\n",
    " Esta función calcula la **matriz de homografía** entre dos conjuntos de puntos, `X` y `x`, mediante un enfoque similar a la **transformación directa lineal** (DLT). La homografía permite alinear una imagen con respecto a otra basándose en los puntos coincidentes.\n",
    "\n",
    " ### Parámetros\n",
    " - `X`: Coordenadas de los puntos en la imagen de referencia.\n",
    " - `x`: Coordenadas de los puntos en la imagen a transformar.\n",
    " - `iterations`: Número de iteraciones para la selección aleatoria de puntos (por defecto 500).\n",
    " - `thresh`: Umbral para determinar qué puntos cumplen con la homografía (por defecto 5).\n",
    "\n",
    " ### Flujo\n",
    " 1. Selecciona aleatoriamente 4 puntos para calcular una homografía temporal.\n",
    " 2. Calcula la matriz de homografía usando la descomposición en valores singulares (SVD).\n",
    " 3. Aplica la homografía calculada a todos los puntos para verificar cuántos cumplen con el umbral.\n",
    " 4. Guarda la mejor matriz de homografía que maximiza el número de puntos coincidentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb0c0a-c533-4c04-99de-2fa6a0d17702",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def getHomo(X, x, iterations=500, thresh=5):\n",
    "    maxCount = 0  # Inicializar contador máximo de coincidencias\n",
    "    for i in range(iterations):\n",
    "        # Selección aleatoria de 4 puntos\n",
    "        idx = np.random.randint(X.shape[0], size=4)\n",
    "        temp_X = X[idx]\n",
    "        temp_x = x[idx]\n",
    "\n",
    "        # Construcción de la matriz para el cálculo de homografía\n",
    "        ax = np.concatenate((-temp_X, np.zeros((4, 3)), temp_x[:, 0:1] * temp_X), axis=1)\n",
    "        ay = np.concatenate((np.zeros((4, 3)), -temp_X, temp_x[:, 1:] * temp_X), axis=1)\n",
    "        M = np.concatenate((ax, ay), axis=0)\n",
    "        \n",
    "        # Descomposición en valores singulares (SVD) para calcular la homografía\n",
    "        u, s, v = np.linalg.svd(M)\n",
    "        H = v[8].reshape(3, 3)\n",
    "        \n",
    "        # Comprobación de cuántos puntos cumplen la homografía\n",
    "        res = X.dot(H.T)\n",
    "        res = np.divide(res, res[:, 2].reshape(-1, 1))  # Normalización\n",
    "        err = np.linalg.norm(res[:, :2] - x, axis=1)\n",
    "        tempCount = np.count_nonzero(err < thresh)\n",
    "\n",
    "        # Guardar la mejor homografía\n",
    "        if tempCount > maxCount:\n",
    "            bestH = H\n",
    "            maxCount = tempCount\n",
    "    \n",
    "    return bestH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773b2f0a-1768-48f9-9259-15603866474b",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Función `keypts`: Detección de Puntos Clave y Coincidencias\n",
    "\n",
    " La función `keypts` detecta los puntos clave en dos imágenes utilizando **SIFT** y encuentra coincidencias sólidas entre ambas mediante el método de **prueba de razón**.\n",
    "\n",
    " ### Parámetros\n",
    " - `im1`: Primera imagen en escala de grises.\n",
    " - `im2`: Segunda imagen en escala de grises.\n",
    " - `ratio`: Ratio de prueba para filtrar coincidencias (por defecto 0.4).\n",
    "\n",
    " ### Salida\n",
    " - Puntos clave de `im1`.\n",
    " - Puntos clave de `im2`.\n",
    " - Lista de coincidencias válidas entre las imágenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5eb2ab5-39bc-46bd-97b1-64a195512add",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def keypts(im1, im2, ratio=0.4):\n",
    "    # Detección de puntos clave con SIFT\n",
    "    sift = cv2.xfeatures2d.SIFT_create(200)\n",
    "    kp1, desc1 = sift.detectAndCompute(im1, None)\n",
    "    kp2, desc2 = sift.detectAndCompute(im2, None)\n",
    "    \n",
    "    # Coincidencia con BFMatcher y prueba de razón\n",
    "    bf = cv2.BFMatcher()\n",
    "    matches = bf.knnMatch(desc1, desc2, k=2)\n",
    "    \n",
    "    goodMatches = []\n",
    "    for m, n in matches:\n",
    "        if (m.distance / n.distance) < ratio:\n",
    "            goodMatches.append(m)\n",
    "    \n",
    "    return kp1, kp2, goodMatches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44849f76-8661-4442-9698-fc9a36d0f552",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Función `stitch`: Unión de Imágenes\n",
    "\n",
    " La función `stitch` une dos imágenes. Primero detecta puntos clave y coincidencias, luego calcula la homografía para transformar una imagen con respecto a la otra.\n",
    "\n",
    " ### Parámetros\n",
    " - `im1`: Imagen de referencia (base).\n",
    " - `im2`: Imagen a transformar.\n",
    "\n",
    " ### Salida\n",
    " - Imagen resultante de la combinación de `im1` e `im2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8fbaa-d588-4416-a5f0-f430ff5a00de",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def stitch(im1, im2):\n",
    "    # Expandir im1 para permitir la unión\n",
    "    im1 = np.pad(im1, ((im1.shape[0], im1.shape[0]), (im1.shape[1], im1.shape[1]), (0, 0)), 'constant')\n",
    "    \n",
    "    # Detectar y coincidir puntos clave entre ambas imágenes\n",
    "    kp1, kp2, goodMatches = keypts(im1, im2)\n",
    "    \n",
    "    # Extraer puntos de coincidencia para calcular la homografía\n",
    "    src_pts = np.array([[kp2[m.trainIdx].pt[0], kp2[m.trainIdx].pt[1]] for m in goodMatches])\n",
    "    dst_pts = np.array([[kp1[m.queryIdx].pt[0], kp1[m.queryIdx].pt[1]] for m in goodMatches])\n",
    "    src_pts = np.concatenate((src_pts, np.ones((src_pts.shape[0], 1))), axis=1)\n",
    "    \n",
    "    # Calcular la homografía y aplicar la transformación perspectiva\n",
    "    h = getHomo(src_pts, dst_pts)\n",
    "    warped = cv2.warpPerspective(im2, h, (im1.shape[1], im1.shape[0]))\n",
    "    \n",
    "    # Combinar las imágenes transformadas\n",
    "    warped[im1 != 0] = 0\n",
    "    final = cleanse(warped + im1)\n",
    "    \n",
    "    return final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fc37b-8dbe-4555-a5f5-30e212b6a307",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Función `cleanse`: Recorte de Bordes\n",
    "\n",
    " La función `cleanse` recorta los bordes de la imagen resultante para eliminar áreas negras, mejorando la apariencia del mosaico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2156c5-4d7e-4a33-9d91-7acf28bc110d",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def cleanse(im):\n",
    "    for i in range(im.shape[1] - 1, 0, -1):\n",
    "        if (im[:, i] != 0).any():\n",
    "            im = im[:, :i]\n",
    "            break\n",
    "    for i in range(0, im.shape[1]):\n",
    "        if (im[:, i] != 0).any():\n",
    "            im = im[:, i:]\n",
    "            break\n",
    "    for i in range(0, im.shape[0]):\n",
    "        if (im[i] != 0).any():\n",
    "            im = im[i:]\n",
    "            break\n",
    "    for i in range(im.shape[0] - 1, 0, -1):\n",
    "        if (im[i] != 0).any():\n",
    "            im = im[:i]\n",
    "            break\n",
    "    \n",
    "    return im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22bb17-f4b2-40fc-8cab-3789f10b002e",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Función `findRoot`: Identificación de Imagen Base\n",
    "\n",
    " `findRoot` encuentra la mejor imagen de referencia para construir el mosaico. Esta imagen base tiene la mayor cantidad de coincidencias con las demás imágenes, asegurando una mayor calidad en el mosaico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ef51e1-1544-4ace-88e4-aa8658bbefa8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "def findRoot(images):\n",
    "    numOfFeats = []\n",
    "    for i in range(len(images)):\n",
    "        temp = 0\n",
    "        for j in range(len(images)):\n",
    "            if i != j:\n",
    "                kp1, kp2, goodMatches = keypts(images[i], images[j])\n",
    "                temp += len(goodMatches)\n",
    "        numOfFeats.append(temp)\n",
    "    \n",
    "    return np.argmax(np.array(numOfFeats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca82606-3192-4811-af55-414cb487958c",
   "metadata": {},
   "source": [
    "# %% [markdown]\n",
    "\n",
    " ## Ejecución del Mosaico\n",
    "\n",
    " Se cargan las imágenes y se organiza el orden de combinación. Luego, se aplica la función `stitch` para unirlas y construir el mosaico final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4a2900-d960-4515-90df-2816f6db1b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCell was canceled due to an error in a previous cell."
     ]
    }
   ],
   "source": [
    "# %%\n",
    "\n",
    "# Definir el orden de las imágenes a cargar\n",
    "order = [2, 1, 4, 3, 6, 5]\n",
    "images = [cv2.imread('images/img2_' + str(i) + '.png') for i in order]\n",
    "\n",
    "# Identificar la imagen base para el mosaico\n",
    "root = findRoot(images)\n",
    "images[0], images[root] = images[root], images[0]\n",
    "\n",
    "# Unión iterativa de imágenes en el orden seleccionado\n",
    "l = len(images)\n",
    "plt.subplot(l, 1, 1)\n",
    "plt.imshow(cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB))\n",
    "for i in range(l - 1):\n",
    "    maxLen = 0\n",
    "    bestMatch = -1\n",
    "    for j in range(1, len(images)):\n",
    "        kp1, kp2, goodMatches = keypts(images[0], images[j])\n",
    "        if len(goodMatches) > maxLen:\n",
    "            maxLen = len(goodMatches)\n",
    "            bestMatch = j\n",
    "    \n",
    "    images[0] = stitch(images[0], images[bestMatch])\n",
    "    images.pop(bestMatch)\n",
    "    \n",
    "    plt.subplot(l, 1, i + 2)\n",
    "    plt.imshow(cv2.cvtColor(images[0], cv2.COLOR_BGR2RGB))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
